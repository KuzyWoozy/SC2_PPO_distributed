## Supervisor Meeting 7
**Date:** 5th June 2023

**Location:** Bayes Center Room 2.02

**Next meeting:** 12th June

### Attendees:
* Dr Evgenij Belikov
* Daniil Kuznetsov

### Discussion:
* Implemented a synchronous stocastic gradient descent (SGD) approach of parallelism entailing an all reduce of network parameters after each episode evaluation.
* Next step is to extend the syncrhonous SGD with hardware, making use of GPU on the Cirrus supercomputer. An alternative would be to consider implementing asyncrhonous SGD approach, which should better suit the dynamic reinforcement learning environment, however would require ARCHER2 for proper evaluation which is in the given moment under maintenence. 
* The PPO algorithm was evaluated for 10 hours in parallel, but this was not enought to beat the DeepMind baseline consistently, although there were observed cases where it did happen (luck).
* Discussed how it would be interesting to evalute performance benefits of Distributed Data Parallel, a pytorch Allreduce wrapper that claims to achieve better communication performance via computation overlapping.
* Discussed how fixing a random seed (or a set) and max agent steps would allow us to deterministictly perform scalability studies on the sync/async SGD with or without GPUs etc.
* Strong and weak scaling can thus be defined by fixing or varying max agent steps within an envrionment across various set random seeds.

### Previous Meeting Actions:
* Implement regression testing to ensure testable development of new features for PPO. (Completed)
* Evaluate the PPO algorithm on the SCII environment for a longer duration, see if we can reach the DeepMind baseline. (Completed)
* Start working of parallelizing PPO, begin with torch MPI functionality. (Completed)

### Actions:
* Extend synchronous SGD with a CUDA compatable GPUs on Cirrus, and evaluate performance improvement if any.
* Look into evaluate performance effect of Torch DDP Allreduce model.
* Begin setting up an experiment environment with plot/graph capabilities.

