## Supervisor Meeting 8
**Date:** 12th June 2023

**Location:** Bayes Center Room 2.02

**Next meeting:** 19th June

### Attendees:
* Dr Evgenij Belikov
* Daniil Kuznetsov

### Discussion:
* Successfully integerated CUDA and observed speedups after minimizing the communication that takes place between host and target, as well as modifying the algorithm to better suit the model.
* Local machine has been damaged and requires repair which will impact work schedule for the week, no critical data related to the project has been lost because of frequent gitlab pushes.
* Experimented with JIT compilation provided by PyTorch however has been unsuccessful in obtaining performance gains, this could be related to the dynamic nature of the RL context or to do with my implementations. Used snakeviz profiling tool to attempt to diagnose if the JIT has a positive impact on certain methods and while there has been evidence to show that it has, I have not been able to observe this in practise (could be related to the short-ish experiment of 5 minutes that is biased by JIT compilation time or perhaps there is overhead occuring that is above the scope of the executing function which could be posssible). The JIT component of PyTorch is still in active development and is not industry ready, hence the issues could stem from elsewhere.
* Next step would be to set up distributed GPU training, and tie in all the work that has been done so far, which will allow scalability testing to begin, given that ARCHER2 is back up and running.

### Previous Meeting Actions:
* Extend synchronous SGD with a CUDA compatable GPUs on Cirrus, and evaluate performance improvement if any. (CUDA extended, not tested on Cirrus yet)
* Look into evaluate performance effect of Torch DDP Allreduce model. (Not done, postponed due to focus on CUDA)
* Begin setting up an experiment environment with plot/graph capabilities. (Not donem postponed due to focus on CUDA)

* Found a way to identify CUDA sync points within PyTorch and was used to optimize communication and obtain speedups on larger networks on a local machine.

### Actions:
* Evaluate performane of the CUDA implementation on Cirrus GPU node, experiment with JIT Dynamo compilation and see if it can be used to obtain performance benefits.
* Begin considering the experiments that will need to be undertaken for sensible evaluation of PyTorch scalability.
* Investigate the suitability of mixed precision for the CUDA implementation and its speedup benefits if any.

